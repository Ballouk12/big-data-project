# Big Data Weather Pipeline: Real-time Ingestion, ETL & Machine Learning

A comprehensive big data project featuring a robust real-time data pipeline, lambda architecture for batch and speed layers, and integrated Machine Learning for weather prediction.

---

## ğŸ“‹ Overview

This project provides a complete end-to-end data platform:

1.  **Real-time Data Ingestion**: Automated collection of weather data using Kafka and Flume.
2.  **Hybrid Processing Architecture**: Leveraging Apache Spark for both streaming inference and batch model training.
3.  **Scalable Storage**: Distributed storage using Hadoop HDFS and NoSQL capabilities with HBase.
4.  **Interactive Analytics**: Zeppelin notebooks for data visualization and model management.

# Architecture of our project

<!-- Replace with your specific architecture execution flow if different -->
<img width="1024" alt="Project Architecture" src="https://github.com/user-attachments/assets/placeholder-architecture" />

## ğŸ—ï¸ Project Structure

```
big-data/
â”œâ”€â”€ producer/                  # Data Ingestion Layer
â”‚   â”œâ”€â”€ producer_kafka.py      # Real-time Open-Meteo fetcher
â”‚   â”œâ”€â”€ extract_final.py       # Batch data extraction script
â”‚   â””â”€â”€ Dockerfile             # Custom Producer image
â”‚
â”œâ”€â”€ spark_apps/                # Spark Processing Layer
â”‚   â”œâ”€â”€ realtime_inference.py  # Structured Streaming & ML Inference
â”‚   
â”‚        
â”‚
â”œâ”€â”€ flume/                     # Log Aggregation
â”‚   â””â”€â”€ conf/                  # Flume agent configuration
â”‚
â”œâ”€â”€ zeppelin/                  # Analytics & Interactive
â”‚   â””â”€â”€ notebooks/             # Data exploration notebooks
â”‚
â”œâ”€â”€ scripts/                   # Utility Scripts
â”‚   â””â”€â”€ init_hbase.txt         # HBase schema initialization
â”‚
â”œâ”€â”€ docker-compose.yml         # Container Orchestration
â””â”€â”€ deploy.sh                  # Deployment automation
```

## ğŸ“š Dataset

This project consumes live data from the **Open-Meteo API**:
ğŸ‘‰ [Open-Meteo Free Weather API](https://open-meteo.com/)

**Description:**
> The pipeline fetches real-time meteorological data including temperature, wind speed, weather codes, and more. This data is used effectively to simulate a continuous stream of IoT sensor data for Big Data processing.

**Key Features:**
- Real-time updates (Temperature, Wind Speed, Weather Code)
- Geolocation-based forecasting
- High-frequency data simulating sensor streams
- Suitable for Time-series forecasting and Anomaly detection

## ğŸš€ Quick Start

### Prerequisites

- Docker Engine 20.10+
- Docker Compose 2.0+
- 10GB RAM minimum (16GB recommended for full stack)

### Setting Up the Pipeline

1.  **Clone the repository**
    ```bash
    git clone https://github.com/Ballouk12/big-data-project.git
    cd big-data
    ```

2.  **Launch the Services**
    The entire stack is containerized. Start it with one command:
    ```bash
    docker-compose up -d
    ```

3.  **Verify Services**
    - **Namenode**: http://localhost:9870
    - **Spark Master**: http://localhost:8080
    - **Zeppelin**: http://localhost:9999 (mapped from internal 8080)

### Running the Workflows

1.  **Start Data Ingestion (Producer)**
    The producer is configured to restart always, but you can view logs or restart manually:
    ```bash
    docker-compose restart producer-kafka
    # View logs
    docker-compose logs -f kafka-producer
    ```

2.  **Submit Spark Streaming Job**
    Connect to the Spark Master container to submit the inference job:
    ```bash
    docker exec -it spark-master-bd bash
    
    # Submit the Real-time Inference Job
    spark-submit \
      --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.2 \
      /opt/spark-apps/realtime_inference.py
    ```

3.  **Train/Retrain Model**
    ```bash
    # GO to Zeeplin interface and run the TRAINING_DATASET_ML notebook
    ```

## ğŸ“Š Pipeline Components & Performance

The project implements a modern Lambda Architecture:

-   **Speed Layer (Spark Structured Streaming)**:
    -   Consumes immediately from `weather-live` Kafka topic.
    -   Applies pre-trained Random Forest models.
    -   Latency: < 500ms per micro-batch.

-   **Batch Layer (Hadoop/HBase)**:
    -   Persists historical data in HBase `weather_history` table.
    -   Supports complex batch analytics and model retraining.

-   **Orchestration**:
    -   Docker Compose manages dependency graphs (Zookeeper -> Kafka -> Spark).

## ğŸ”§ Features

-   **Fault Tolerance**: Kafka buffering ensures no data loss during processing spikes.
-   **Distributed ML**: Spark MLlib used for distributed training and inference.
-   **NoSQL Integration**: High-throughput writes to HBase for historical archiving.
-   **Dynamic Dashboarding**: Zeppelin notebooks connected to Spark context for live visualization.

## ğŸ“¦ Data Management

### Supported Formats & Storage
-   **Apache Kafka**: JSON serialized messages (Topics: `weather-live`)
-   **HBase**: Key-Value storage for infinite scaling.
-   **HDFS**: Checkpointing and model artifacts.

### Data Directories (Docker Volumes)
```
volumes/
â”œâ”€â”€ hbase-data/       # Persistent HBase storage
â”œâ”€â”€ namenode/         # HDFS NameNode metadata
â”œâ”€â”€ datanode/         # HDFS Data blocks
â””â”€â”€ shared-data/      # Shared artifacts between containers
```

## ğŸ› ï¸ Development

### Adding a New Feature

1.  **Modify the Producer**:
    Edit `producer/producer_kafka.py` to add new fields from the API.

2.  **Deploy**:
    Restart the specific containers to apply changes:
    ```bash
    docker-compose restart kafka-producer spark-worker
    ```

## ğŸ“ˆ Monitoring & Troubleshooting

### View Logs
```bash
# Kafka Producer logs (Check data flow)
docker-compose logs -f kafka-producer

# Spark Worker logs (Check job execution)
docker-compose logs -f spark-worker
```

### Common Issues

**"Connection Refused" to HBase**:
Ensure the HBase container is fully healthy before starting consumers.
```bash
docker-compose ps hbase-standalone
```

**Spark "Class Not Found"**:
Ensure you include the Kafka package when submitting jobs:
`--packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.2`

## ğŸ“š Documentation

-   [Apache Spark Streaming Guide](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html)
-   [Open-Meteo API Docs](https://open-meteo.com/en/docs)
-   [HBase Reference Guide](https://hbase.apache.org/book.html)

## ğŸ¤ Contributing

Contributions are welcome! Please:

1.  Fork the repository
2.  Create a feature branch (`git checkout -b feature/amazing-feature`)
3.  Commit your changes (`git commit -m 'Add amazing feature'`)
4.  Push to the branch (`git push origin feature/amazing-feature`)
5.  Open a Pull Request

## ğŸ“ License

This project is licensed under the MIT License.

## ğŸ‘¥ Authors

-   **Ballouk Mohamed** - [GitHub Profile](https://github.com/Ballouk12)
-   **Sakhr Niama**
-   **Boukhrais Meryem**

## ğŸ™ Acknowledgments

-   Apache Software Foundation for open-source big data tools.
-   Docker community for containerization resources.
-   Open-Meteo for the excellent free weather API.

---

**â­ Star this repository if you find it helpful!**
